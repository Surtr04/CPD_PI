\section{Conclusions}
\label{sec:conclusions}

In this document, a parallel implementation of the \texttt{polu} program was presented and analyzed.

The main problem -- partitioning the mesh -- was solved by a naive, yet functional implementation. Some works were found which specialized in this topic, but could not be used due to time (and consequently, complexity) constraints. The implementation using vertically sliced partitions was described, from the data structures used control which data is transmitted and what is redundant to the communication itself.

Performed the implementation, a light profiling was performed to compare this version with the previous results in this project. The best results were achieved using 1 process per core in the SeARCH Group 101 nodes and using 2 processes per core in the SeARCH Group Hex nodes. Speedups were possible, reaching 9.36 using 48 processes in two Group Hex nodes, and increased linearly in the Group Hex nodes until the hardware limit was crossed. The behavior of the program in the two groups were quite distinct, but the bottlenecks were not. For the 2,000 iterations performed in each test in this document, the partitioning overhead achieved almost 85\% of the total execution time. Ignoring the pre-computational steps, the communication was the main bottleneck, representing 98\% of the executing time in the worst case. The results indicate that this implementation is communication-bound, but its scalability also depends greatly on the hardware (number of nodes used and processors) and the number of processes issued.

Further testing with this implementation was not performed due to excessive job traffic and software problems in the SeARCH cluster. This would be useful to understand better the potential in this version, for instance, to find out how the results would vary using more nodes of the SeARCH Group Hex. Also, several profilers exist for MPI (\texttt{vampirtrace}, \texttt{mpiP}, \texttt{TAU}), which would give a more complete picture of the program's execution, but such either were not available in the cluster, or some questions about the correct configuration prevented their use.

Future work in the MPI version (not aimed for this project) would include reinforcing the measurement codes (using the mentioned profilers, for example), optimizing the communication step (studying a possible overlap of computation and communication) or the partitioning of the mesh (while the impact of this step is reduced by increasing the number of iterations, it is still a very heavy and slow procedure).
