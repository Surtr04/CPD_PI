\section{Conclusion}
\label{sec:conclusion}

In this report, the analysis of the \polu application and the successive attempts to optimize and parallelize it were presented.

The original implementation presented several performance problems, the most troubling being the structures implemented as \aop. It also presented features which were not fully implemented or not targeted for improving the results of the computation. These features were removed. The main optimizations performed in the sequential implementation involved changing how the structures were implemented to \aos, and then to \soa, which achieved the best results (almost 10 times faster).
Dependencies existed in the original code, which were removed along with the discarded features and some code adaptations.

Several approaches to parallelism were described. The first implementation, shared memory, used the OpenMP interface to parallelize the two core functions. It achieved a nearly perfect load balance but remained, just like the sequential version, highly limited by the lack of locality in the mesh structure. The \soa version also achieved the best results with this implementation ()

\todopfac{Value missing bitch}

The second implementation described in this document uses distributed memory with MPI. The main problem encountered while developing this implementation was found in the mesh partitioning scheme. While other options promised better results, time restrictions could prevent from achieving a functional version. A naive approach was taken, which optimized the computational workload while neglecting the communication overhead. The communication became the overhead of the program, surpassing locality. The best results were obtained using the two complete nodes, taking advantage of hyper-threading. Such can be explained by the existence of separate memory banks and the smaller partitions.

Lastly, CUDA was used to implement a massively parallel approach using GPUs. This implementation and the shared memory one were very similar, and took advantage of the same simplifications and optimizations. Two versions were designed: a naive one, obtained by translating the sequential code to CUDA and implementing some optimized kernels (a reduction which was later removed by the simplifications); and a second optimized version, which aimed to reduce the number of memory accesses and branch divergences. This last version obtained the best results in the entire project (34 times faster than the original implementation).

While CUDA obtained the best results, a more significant difference was expected. Further testing with larger test cases is required to fully take advantage of the hardware capabilities. Yet, such case could not have been generated in useful time.

Among the several problems encountered while developing this code, the lack of locality of the mesh was the most relevant. It affected all the implementations here described. Works of other authors were found, and libraries exist to improve this issue, but their application was discarded due to time constrictions. These approaches are left as future work.
