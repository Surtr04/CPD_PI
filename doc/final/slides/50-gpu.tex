\section{GPU}

\frame{\frametitle{Index}\tableofcontents[currentsection]}

%
% IMPLEMENTATION
%
\frame{\frametitle{GPU}

	\begin{block}{Implementation}
		\begin{itemize}\itemsep=20pt
			\item \textit{Struct-of-Arrays}
			\pause

			\item \texttt{compute\_flux} and \texttt{update} directly converted to CUDA Kernels
			\begin{itemize}
				\item[-] Each iteration mapped to one CUDA Thread
			\end{itemize}
			\pause

			\item One aditional \texttt{reduction} kernel (later removed)
			\pause

			\item Data transfers only happen before the main loop
			\pause
			\begin{itemize}
				\item[] (exception: animation output, which is removed)
			\end{itemize}
		\end{itemize}
	\end{block}
}

\frame{\frametitle{GPU}
	\begin{block}{Load Balance}
		\begin{itemize}\itemsep=20pt
			\item workload of both kernels is homogeneous
			\pause
			\begin{itemize}
				\item[] (exception: for \update kernel, if number of edges per cell differs)
			\end{itemize}
			\pause
		\end{itemize}
	\end{block}

	\begin{block}{Limitations}
		\begin{itemize}
			\item Memory accesses are the main issue
			\pause
			\begin{itemize}
				\item[-] \computeflux will not access cells in order
				\item[-] Same problem for \update when accessing edges
			\end{itemize}

		\end{itemize}
	\end{block}
}

