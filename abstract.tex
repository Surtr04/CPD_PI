\documentclass[9pt,twocolumn]{scrartcl}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

%\usepackage{graphics}%	images other than eps

\usepackage[paper=a4paper,top=2cm,left=1.5cm,right=1.5cm,bottom=2cm,foot=1cm]{geometry}

\usepackage{relsize}%	relative font sizes

%\usepackage[retainorgcmds]{IEEtrantools}%	IEEEeqnarray

\usepackage{hyperref}

\usepackage{listings}
\usepackage{color}

%%%%%%%%%%%%%%%%
%  title page  %
%%%%%%%%%%%%%%%%
\titlehead{Universidade do Minho \hfill Parallel and Distributed Computing\\	Master's Degree in Informatics Engineering}

\title{Profiling + CUDA}

\subtitle{A finite volume case study from an industrial application\\(Extended Abstract)}%	TODO รณ Naps, did lรก o nome dest a merda!!

\author{Miguel Palhas \hfill--- \texttt{\smaller pg19808@alunos.uminho.pt}\\Pedro Costa \hfill--- \texttt{\smaller pg19830@alunos.uminho.pt}}

\date{Braga, January 2012}

\subject{Integrated Project}


%%%%%%%%%%%
%  Hacks  %
%%%%%%%%%%%

%	Paragraph with linebreak
\newcommand{\paragraphh}[1]{\paragraph{#1\hfill}\hfill

}


\begin{document}
\maketitle

\section{Introduction}
\subsection{Contextualization}
The Finite Volumes Method (FVM) is one of the three classical choices for solving PDEs\footnote{Partial Differential Equations: equations involving functions with more than one variable and their partial derivatives.} numerically, together with the Finite Difference Method (FDM) and the Finite Element Method (FEM).

The FDM is the oldest method and is based in the classical formal definition of a function's derivative in order to a parameter $x$ as the limit of that function's average rate as the difference between the two points tends towards zero ($\Delta x \rightarrow 0$).

%$${\left . u_{x} \right |}_{i} = u_{x} \left ( x_{i} \right ) = \lim_{\Delta x \rightarrow 0}{\frac{u \left ( x_{i} + \Delta x \right ) - u \left ( x_{i} \right )}{\Delta x}}$$

Even if $\Delta x$ is not constant throughout the mesh, this forces the need for it to be structured on the FDM. The FEM and FVM overcome this by discretizing based on the integral form of the governing equations, therefore being able to handle complex geometries in multi-dimensional problems.

%TODO: FEM

%TODO: FVM

%TODO: fluids mechanics & pollution spreading

\subsection{Motivation}% what we intend to achieve
This document analyzes an application which computes the evolution of a pollutant in a given environment, studying possibilities of optimization and/or parallelization.

Two attempts of parallelization are intended: one using shared memory on a multiprocessor system and one using a GPU.

The final aim is to have a qualitative and quantitive comparison of both versions against each other and the original sequential code, and an evaluation of possible bottlenecks and/or optimizable code parts.

\subsection{Structure}% what will we do (list of actions), a.k.a. structure
The initial profiling of the application, including the methodology and conclusions, is presented in \autoref{sec:initprof}. In \autoref{sec:optm&para}, the initial profiling information is used to identify the parts of the algorithm/code which are better suited for optimization and/or parallelization. The implementation of a shared memory parallel version and a GPU version are explained in \autoref{sec:openmp}  and \autoref{sec:cuda}, respectively.

\section{Sequential}% why did we choose compute_flux & update to mess with
\label{sec:initprof}
The analysis of the sequential code starts with the generation of it's callgraph, from which it is possible to find the function where the program execution spends most of it's time. According to Amdahl's Law, this function should be the first to study for optimizations and/or parallelization, as it's where one can profit the most from speedup.

For the purpose of generating the call graph, the \texttt{Callgrind} tool\footnote{From the Valgrind suite.} was used. The information generated by this tool could then be visualized using \texttt{KCachegrind}.

Originally this reported 11\% of the execution time in the \texttt{compute\_flux} function and 8\% in the \texttt{update}, while the remaining\footnote{A very small percentage of the time is spent on other functions, which can therefore be ignored} 80\% of the time was spent with I/O (to build an animation). Since the goal is optimize the computation, the I/O calls were deactivated and the profiling tool reported 63\% of the time in the \texttt{compute\_flux} and the remaining time in the \texttt{update}.

\section{Optimization \& Parallelization}
\label{sec:optm&para}
As determined in \autoref{sec:initprof}, the best targets for optimization and/or parallelization are the \texttt{compute\_flux} and \texttt{update} functions, which will be the primary and secondary goals for this project, respectively (each parallelized version will be focusing on these functions by this order).

The first optimization possible in the \texttt{compute\_flux} function is the removal of the \texttt{Parameter.getDouble()} call. This was meant to retrieve the Dirichlet condition value from the parameter file, but being this a constant throughout the program execution, the value can be loaded in an early preparation stage and sent as an argument to \texttt{compute\_flux}. Additionally, moving this argument to a global variable can reduce the function call overhead.

Excessive dereferencing may be common problem on both functions and is directly related to the FVLib library. Although the usage of pointers in classes may greatly improve code readability, the multiple memory accesses caused by deep levels of dereferencing tend to aggravate effects of any memory bottlenecks. With some adjustments to the library internal structures, the same or better readability could be achieved with fewer pointers and lower reference depth.

Lastly, to allow the parallelization in both intended approaches, the loops themselves have to be changed both in \texttt{compute\_flux} and \texttt{update}. The original code uses a non-standard iterator-like approach to loops, where the container object is told to begin the iteration and then returns the elements one by one, iterating automatically through them. While this works in sequential code, it won't work with conventional parallelization mechanisms, as there would be no knowledge of the iterations other than the data itself (which is not enough). This mechanisms require index based accesses, or at least random-access iterators\footnote{OpenMP works with both, CUDA works naturally with indexes.}. Since the FVLib library implements index based accesses, the changes in the loops are trivial.

\subsection{Dependecies}
In the \texttt{compute\_flux} function code one can easily find a data dependency which prevents parallelization. This dependency is found in the calculation of $\Delta t_{i}$ (the final value $\Delta t_{n}$ is returned by the function) and causes each iteration to depend on the previous one (the first iteration uses a preset value $\Delta t_{0}$).

Using mathematical operations this calculation can be removed from the loop and the dependence substituted by a calculation of the maximum computed velocity ($v_{max}$), which can be computed through reduction.

The \texttt{update} function also holds a data dependency between some iterations. This function iterates through every edge, changing the pollution values in every cell based on the computed velocity value of each of it's edges. Since any cell has more than one edge and the final value in a cell is the sum of the contributions from all it's edges, the value can be changed in more than one iteration.

It's possible to remove this dependency by changing the loop itself to iterate over each cell calculating the final value from all it's edges. Considering that no edge data is changed, this completely removes de dependency and allows the parallelization of the function.

\section{OpenMP}
\label{sec:openmp}
\subsection{Implementation}
The main feature about OpenMP that makes it so interesting to create shared memory parallel code is that the program itself is almost equal to the sequential version, with the addition of the required directives and adaptation tweaks (mainly to solve concurrency problems).

Because there are no nested parallelizable loops in this code, the number of threads to be issued are set by default to the maximum number of threads supported by the hardware to be executing at any given time. Despite that, this number is multiplied by a factor (1 by default) to allow experiences to be easily conducted using more threads. The goal of this approach is to achieve an effect similar to the one in GPUs: theoretically, issuing more threads than those the hardware can execute in parallel may improve efficiency by allowing any thread to step in while another is stalled on any resource.

\subsubsection{Flux Computation}
As expected, the parallelization of the \texttt{compute\_flux} function is very straight-forward with the exception of the maximum computed velocity computation. This problem is discussed next, and is therefore ignored here.

To parallelize this function, a \texttt{parallel for} OpenMP directive is placed before the inner loop. Considering all the variables external to the parallel zone as shared, all the required variables must be explicitly made private (either with the \texttt{private({\textit{list}})} clause or by declaring the variables only inside the parallel zone).

\paragraphh{\texttt{max} reduction}
The implementation of the \texttt{max} function reduction is only available in \texttt{gcc} 4.7 (at the date, the standard version is 4.6).

To solve this question without compromising the parallelization, the \texttt{max} function is performed by each thread on it's iterations, storing the values in a common array (prepared at the beginning of the program). After the loop, this local maxima array is sequentially scanned to find the global maximum computed velocity. Although this imposes a limit on the number of threads (which would cause the scan to take longer), it represents a fair compromise.

\subsection{Profiling}
\subsubsection{PAPI framework}% why doesn't PAPI have class in the source FFS
To profile the resulting code with some detail, a framework was implemented to take advantage of C++ features (such as exceptions and objects) when using the PAPI library.

This framework allows to group several events into predefined sets, as long as creating custom sets on the go. Given the changes required to profile OpenMP code, this framework greatly facilitates the library error handling, improving robustness and readability.

The following sets are predefined: CPI (total instructions and cycles), Memory (load and store instructions), Flops/s (floating point operations and total cycles), L1 (first level cache accesses and misses), L2 (same for second level cache), Operational Intensity (RAM bytes accessed and Flops), Instructions Per Byte (RAM bytes accessed and total instructions), Multiplication/Addition Balance (FP total, multiplication and division instructions).

\subsubsection{Methodology}% scripts are cool, talk about them
To perform measurements in the parallel code the PAPI library requires some adaptation. Although the library initialization and cleanup can (and should) be performed in the same places as the sequential code, each thread in a parallel zone must handle it's own event set. Additionally, when initializing the library, the thread support must be explicitly activated.

Once in the parallel zone, each thread is responsible for creating it's own event set (either predefined or custom), starting it before the loop begins, stopping it after while collecting the results (which are stored in a shared array, like the local maxima), and removing the event set before the parallel zone ends. Outside, the global maximum computation is measured in the same way, and the final results are gathered sequentially.


\section{CUDA}
\label{sec:cuda}
\subsection{Data Structures}
\subsection{Implementation}
\subsubsection{Flux Computation}
\paragraphh{\texttt{max} reduction}
\subsubsection{Mesh Update}

\section{Conclusion}
\subsection{Future Work}
%	OpenMP:	update, structures
%	remake FVLib as a proper programmer with a brain would know how


%	Side Notes: OpenFVM.sourceforge.net

\end{document}