\section{CUDA Implementation}
\label{sec:600}

\todosec{CUDA implementation}
Part of the work of this project was to study the parallelization methods and issues of the studied schemes in a massively parallel architecture, specifically a GPU, using CUDA as the development technology. Some considerations can be made about the parallelization of each of the schemes presented.

The first-order scheme is clearly the one where the translation to a massively parallel implementation is straightforward. Each value of the solution at the end of each iteration is being computed based solely on the strictly adjacent elements of the mesh. The higher locality gained from only using adjacent values for each edge allows for much greater locality and less memory overhead than the second-order counterparts.

As for the second-order schemes, such a straightforward solution is not possible, or at least not with the same ratio of performance gain when compared to a first-order scheme. This is due to the more irregular nature of these schemes.
In the reconstruction phase, we need access to the immediate neighbours of each cell. In a two dimensional mesh, the number of neighbours is arbitrary, and it is not possible to perfectly serialize each cell in memory. This results in a higher memory access cost, since we can't take as much advantage of the GPU's coalesced memory accesses at this step.

As for the two second-order approaches tested, while the {\it a posteriori} solution with the MOOD method provides a better numerical result, it also comes with a performance cost, when compared to the MUSCL method. This is mostly due to the recalculations that are necessary in the inner computations of the correct flux, which is iteratively updated until there are no more problematic values. This introduces a bottleneck, particularly noticeable in a parallel environment, since every successive iteration will only deal with a smaller amount of data than the previous one, as the amount of invalid values is reduced.
In the MUSCL method we also have a higher amount of divergence than the first-order scheme. This is not as significant as in the MOOD method, however.
Both of these conclusions seem to suggest that the {\it a priori} approach allows for a better, more regular parallelization in a massively parallel architecture.

Some more elaborate optimizations have been proposed for the second-order schemes, which might help achieve performances near the first-order scheme. One possible optimization would be to partition the mesh, allowing different chunks to be processed independently. This would allow chunks that have already been completely processed to advance further in time without waiting for the global iteration to finish, and would possibly help hide the latency from the MOOD method value correction. Since the amount of values to fix is usually small, having different partitions in different states would allow for partitions that had already been completely corrected to continue on, and effectively increase the usage of the device.
Another possibility would be the addition of coherence control, by controling the serialization of the mesh, and the cells being processed at each time. The application does not perform any preprocessing on the mesh organization, thus relying on the mesh generator to generate a compute-friendly input data. This might result in a mesh layout that does not take into account the advantages that can be gained in terms of performance by exploiting locality. A strategy that would reorder the mesh by maximizing the correlation between the physical element distance and their distance in the memory space would increase locality and consequentely, performance on all employed methods, including the first-order scheme.
